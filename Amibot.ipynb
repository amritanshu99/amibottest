{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c4703d-9222-4575-bfac-9347f95ae6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\amrit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\amrit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\amrit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\amrit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\amrit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading transformer model...\n",
      "✅ Loaded CSV with encoding: Index(['Field', 'Value'], dtype='object')\n",
      "💾 Saved df.pkl, field_variants.pkl, and field_embeddings.pt to 'amibot_data/'\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask AmiBot (type 'exit' to quit):  name\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Matched: 'name'\n",
      "📐 Semantic: 0.44, 🔤 Fuzzy: 100.0\n",
      "👉 I’m Amritanshu Mishra — this bot speaks from my data, my words, my world.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask AmiBot (type 'exit' to quit):  wife\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Matched: 'wife'\n",
      "📐 Semantic: 0.81, 🔤 Fuzzy: 100.0\n",
      "👉 Committed for life — merged with Sneha Mishra in a lifelong partnership.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask AmiBot (type 'exit' to quit):  skills\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Matched: 'skills'\n",
      "📐 Semantic: 0.55, 🔤 Fuzzy: 100.0\n",
      "👉 Skilled in MERN Stack, React, Node.js, and advanced AI/ML systems, including Transformers.\n"
     ]
    }
   ],
   "source": [
    "# 📦 Install required packages (run once)\n",
    "!pip install -q sentence-transformers rapidfuzz nltk\n",
    "\n",
    "# 📥 Imports\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from rapidfuzz import fuzz\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "import re\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# 📌 Download NLTK corpus\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# 🧠 Load transformer model\n",
    "print(\"📥 Loading transformer model...\")\n",
    "model = SentenceTransformer(\"paraphrase-MiniLM-L3-v2\")\n",
    "\n",
    "# 📄 Load your CSV file (replace path if needed)\n",
    "csv_path = \"amibot.csv\"  # Ensure it has columns: 'Field', 'Value'\n",
    "try:\n",
    "    df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv(csv_path, encoding='cp1252')  # fallback encoding\n",
    "\n",
    "print(\"✅ Loaded CSV with encoding:\", df.columns)\n",
    "\n",
    "# 📚 Preprocess data\n",
    "field_variants = []\n",
    "field_map = {}\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    field_str = row[\"Field\"]\n",
    "    value = row[\"Value\"]\n",
    "    variants = [v.strip().lower() for v in field_str.split(\",\") if v.strip()]\n",
    "    for v in variants:\n",
    "        field_variants.append(v)\n",
    "        field_map[v] = value  # Map each variant to its value\n",
    "\n",
    "field_embeddings = model.encode(field_variants, convert_to_tensor=True)\n",
    "\n",
    "# 🔧 Function: Correct typos (basic spell fix using regex for now)\n",
    "def correct_typos(text):\n",
    "    text = text.strip().lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "# 🔧 Function: Expand with synonyms using WordNet\n",
    "def expand_with_synonyms(text):\n",
    "    words = text.split()\n",
    "    expanded_words = []\n",
    "    for word in words:\n",
    "        synonyms = set()\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonyms.add(lemma.name().replace(\"_\", \" \"))\n",
    "        if synonyms:\n",
    "            expanded_words.append(word + \" \" + \" \".join(list(synonyms)[:2]))\n",
    "        else:\n",
    "            expanded_words.append(word)\n",
    "    return \" \".join(expanded_words)\n",
    "\n",
    "# 🤖 Function: Get AmiBot response\n",
    "def get_response(user_input, model, field_variants, field_embeddings, field_map, threshold=0.55, fuzz_threshold=55):\n",
    "    original_input = user_input.strip()\n",
    "    corrected_input = correct_typos(original_input)\n",
    "    expanded_input = expand_with_synonyms(corrected_input)\n",
    "\n",
    "    query_embedding = model.encode(expanded_input, convert_to_tensor=True)\n",
    "    similarities = util.cos_sim(query_embedding, field_embeddings)[0]\n",
    "\n",
    "    best_score = float(similarities.max())\n",
    "    best_idx = int(similarities.argmax())\n",
    "    best_field = field_variants[best_idx]\n",
    "    best_answer = field_map[best_field]\n",
    "\n",
    "    fuzzy_score = fuzz.token_set_ratio(original_input.lower(), best_field.lower())\n",
    "\n",
    "    if best_score >= threshold or fuzzy_score >= fuzz_threshold:\n",
    "        return f\"\\n✅ Matched: '{best_field}'\\n📐 Semantic: {best_score:.2f}, 🔤 Fuzzy: {fuzzy_score}\\n👉 {best_answer}\"\n",
    "    else:\n",
    "        return f\"\\n🤖 Sorry, I’m not sure what you meant.\\n💡 Did you mean: '{best_field}'?\\nPlease rephrase your question.\"\n",
    "\n",
    "# 💾 Save necessary components for Flask app\n",
    "save_dir = \"amibot_data\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "with open(f\"{save_dir}/df.pkl\", \"wb\") as f:\n",
    "    pickle.dump(df, f)\n",
    "\n",
    "with open(f\"{save_dir}/field_variants.pkl\", \"wb\") as f:\n",
    "    pickle.dump(field_variants, f)\n",
    "\n",
    "with open(f\"{save_dir}/field_map.pkl\", \"wb\") as f:\n",
    "    pickle.dump(field_map, f)\n",
    "\n",
    "torch.save(field_embeddings, f\"{save_dir}/field_embeddings.pt\")\n",
    "\n",
    "print(\"💾 Saved df.pkl, field_variants.pkl, and field_embeddings.pt to 'amibot_data/'\")\n",
    "\n",
    "# 🧪 Test in Notebook (example)\n",
    "while True:\n",
    "    user_input = input(\"\\nAsk AmiBot (type 'exit' to quit): \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        break\n",
    "    response = get_response(user_input, model, field_variants, field_embeddings, field_map)\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4c90978-973d-412f-a4c7-9dffc42cb2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install & Import Packages\n",
    "# !pip install -q sentence-transformers rapidfuzz nltk\n",
    "# Load the Transformer Model\n",
    "# model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "# This model converts text into dense vector embeddings.\n",
    "\n",
    "# Embeddings capture semantic meaning (not just exact words).\n",
    "\n",
    "# It’s fast and lightweight.\n",
    "# Internally:\n",
    "# \"father name\" → [0.23, -0.14, ..., 0.01] (384-dim vector)\n",
    "# \"What’s your dad's name?\" → similar vector\n",
    "# 3. Read and Parse CSV\n",
    "# Each row in the CSV maps Field(s) → Value (response).\n",
    "# 6. User Input Flow\n",
    "\n",
    "# Comma-separated synonyms like:\n",
    "# \"father name, dad name, papa\" are split and preprocessed.\n",
    "\n",
    "# Preprocess: Extract Field Variants\n",
    "# for row in df:\n",
    "#     variants = field_str.split(\",\")\n",
    "#     for v in variants:\n",
    "#         field_variants.append(v.lower().strip())\n",
    "#         field_map[v.lower()] = value\n",
    "# field_variants = [\"father name\", \"dad name\", \"papa\", \"your name\", ...]\n",
    "# field_map = {\n",
    "#   \"father name\": \"Anshul Sharma\",\n",
    "#   \"papa\": \"Anshul Sharma\",\n",
    "#   ...\n",
    "# }\n",
    "\n",
    "# 5. Generate Embeddings\n",
    "# field_embeddings = model.encode(field_variants, convert_to_tensor=True)\n",
    "# Each field becomes a semantic vector:\n",
    "# \"father name\" → tensor([0.12, -0.55, ..., 0.33])\n",
    "\n",
    "# 6. User Input Flow\n",
    "# def get_response(user_input, ...)\n",
    "# Internally:\n",
    "\n",
    "# a. Correct Typos\n",
    "# correct_typos(\"Dad’s n@me!\") → \"dads name\"\n",
    "# b. Expand with Synonyms\n",
    "# expand_with_synonyms(\"dads name\") → \"dads name dad father\"\n",
    "\n",
    "# c. Encode Input\n",
    "# query_embedding = model.encode(expanded_input)\n",
    "# d. Computes similarity between query and each field:\n",
    "# \"What's your dad’s name?\" vs [\"father name\", \"your name\", ...]\n",
    "# → cosine scores like [0.87, 0.22, 0.04, ...]\n",
    "\n",
    "# e. Fuzzy Score\n",
    "# fuzz.token_set_ratio(\"what’s your dad’s name\", \"father name\") → 80\n",
    "\n",
    "# f. Threshold-Based Match\n",
    "# if similarity > 0.55 or fuzzy_score > 55:\n",
    "#     return correct response\n",
    "# else:\n",
    "#     return \"Sorry, I’m not sure...\"\n",
    "\n",
    "# 7. Save Artifacts for Flask\n",
    "\n",
    "# pickle.dump(df, field_variants, field_map)\n",
    "# torch.save(field_embeddings)\n",
    "# Avoid recomputing embeddings when deploying.\n",
    "\n",
    "# Use this in your Flask app without repeating preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2103bf3-dc98-461b-987b-52f1a8f5bfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | 🔢 Step | 🧩 Component             | 📝 Description                         | 🧠 Internal Operation                                                      | 🧪 Example                                            |\n",
    "# | ------- | ------------------------ | -------------------------------------- | -------------------------------------------------------------------------- | ----------------------------------------------------- |\n",
    "# | 1️⃣     | **Install Packages**     | Installs required libraries            | Downloads and sets up `sentence-transformers`, `rapidfuzz`, and `nltk`     | `pip install -q sentence-transformers rapidfuzz nltk` |\n",
    "# | 2️⃣     | **Import Modules**       | Loads Python packages                  | Imports for NLP, embedding, fuzzy logic, and preprocessing                 | `import pandas as pd`, `import torch`, etc.           |\n",
    "# | 3️⃣     | **Download WordNet**     | Enables synonym expansion              | Downloads NLTK corpora: `wordnet` & `omw-1.4`                              | `nltk.download('wordnet')`                            |\n",
    "# | 4️⃣     | **Load CSV File**        | Loads personal Q\\&A data               | Reads `amibot.csv` into a DataFrame with 'Field' and 'Value' columns       | CSV sample: `father name, Anshul Sharma`              |\n",
    "# | 5️⃣     | **Parse Field Variants** | Splits Field into multiple query forms | For each comma-separated variation in \"Field\", create mappings             | `\"father name, dad name\"` → 2 keys                    |\n",
    "# | 6️⃣     | **Build Mapping Dicts**  | Store phrases and answers              | `field_variants = []` stores queries, `field_map = {}` maps to values      | `\"dad name\" → Anshul Sharma`                          |\n",
    "# | 7️⃣     | **Generate Embeddings**  | Semantic vectors for field variants    | Converts all `field_variants` to dense vectors using `SentenceTransformer` | `\"father name\" → [0.23, -0.54, ..., 0.11]`            |\n",
    "# | 8️⃣     | **Typo Correction**      | Pre-clean user input                   | Removes symbols and lowercases the input via regex                         | `\"Dad’s name?\" → \"dads name\"`                         |\n",
    "# | 9️⃣     | **Synonym Expansion**    | Enhances semantic reach                | Adds 1–2 synonyms from WordNet to each word                                | `\"dad\"` → `\"dad father papa\"`                         |\n",
    "# | 🔟      | **User Input Encoding**  | Transforms input to embedding          | Uses model to encode expanded user input                                   | `\"Who is your dad?\" → tensor`                         |\n",
    "# | 1️⃣1️⃣  | **Cosine Similarity**    | Semantic comparison                    | Measures angle between input vector and all stored field vectors           | `cos_sim = 0.82 with \"father name\"`                   |\n",
    "# | 1️⃣2️⃣  | **Fuzzy Matching**       | Textual string similarity              | Uses `fuzz.token_set_ratio` to score rough matches                         | `\"Who is your dad?\" vs \"father name\" → 76`            |\n",
    "# | 1️⃣3️⃣  | **Response Selection**   | Final decision on best match           | Chooses highest score above thresholds: `cos_sim > 0.55 or fuzzy > 55`     | ✅ Match: `\"father name\" → Anshul Sharma\"`             |\n",
    "# | 1️⃣4️⃣  | **Fallback Message**     | Handles low-match inputs               | Suggests closest match or asks user to rephrase                            | `\"🤖 Sorry, I’m not sure...\"`                         |\n",
    "# | 1️⃣5️⃣  | **Save Artifacts**       | Save all required objects              | Dumps model outputs and mappings to `amibot_data/` folder for Flask        | `field_embeddings.pt`, `field_map.pkl`                |\n",
    "# | 1️⃣6️⃣  | **Interactive Testing**  | Run in Jupyter loop                    | Continuously prompt for input, display match & answer                      | `input(\"Ask AmiBot: \")`                               |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc92fe00-dc24-41b9-9026-571c010b595f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | 🔎 User Input             | 🎯 Cleaned Input        | 🧠 Expanded Input                   | 🔗 Best Match    | 📐 Cosine Sim | 🔤 Fuzzy Score | ✅ Final Response            |\n",
    "# | ------------------------- | ----------------------- | ----------------------------------- | ---------------- | ------------- | -------------- | --------------------------- |\n",
    "# | \"What’s your dad’s name?\" | `whats your dads name`  | `whats your dads name dad father`   | `father name`    | 0.87          | 78             | `Anshul Sharma`             |\n",
    "# | \"Tell me your birthday\"   | `tell me your birthday` | `tell me your birthday natal birth` | `dob`            | 0.74          | 68             | `09 September 1996`         |\n",
    "# | \"Favourite dish?\"         | `favourite dish`        | `favourite dish food meal`          | `favourite food` | 0.51          | 43             | `🤖 Sorry, please rephrase` |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c125251-3dae-40b9-8518-1417c558ce33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | 📁 File               | 📄 Format      | 🧠 Contents                           |\n",
    "# | --------------------- | -------------- | ------------------------------------- |\n",
    "# | `df.pkl`              | Pickle         | Original CSV DataFrame                |\n",
    "# | `field_variants.pkl`  | Pickle         | All phrases extracted from 'Field'    |\n",
    "# | `field_map.pkl`       | Pickle         | Maps each variant → Value             |\n",
    "# | `field_embeddings.pt` | PyTorch Tensor | Vector representation of all variants |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c3862d4-22a9-4726-af6c-d1834c70dca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | Setting               | Description                                 | Recommendation                    |\n",
    "# | --------------------- | ------------------------------------------- | --------------------------------- |\n",
    "# | `threshold = 0.55`    | Minimum cosine similarity to consider match | Lower to 0.5 for broader matches  |\n",
    "# | `fuzz_threshold = 55` | Minimum fuzzy ratio for textual match       | Keep above 50 to avoid false hits |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "683f0bf5-aca5-41a2-8ace-0c6a5206b28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ┌────────────────────────────────────────────┐\n",
    "# │           START AMIBOT SYSTEM              │\n",
    "# └────────────────────────────────────────────┘\n",
    "#                   │\n",
    "#                   ▼\n",
    "# ┌────────────────────────────────────────────┐\n",
    "# │ 🔄 Load CSV File (amibot.csv)              │\n",
    "# │ - Columns: 'Field', 'Value'                │\n",
    "# │ - Example: \"father name, dad name\", \"Anshul Sharma\" │\n",
    "# └────────────────────────────────────────────┘\n",
    "#                   │\n",
    "#                   ▼\n",
    "# ┌────────────────────────────────────────────┐\n",
    "# │ 🔄 Parse and Preprocess Fields              │\n",
    "# │ - Split comma-separated fields             │\n",
    "# │ - Store in:                                │\n",
    "# │     • field_variants (list of queries)     │\n",
    "# │     • field_map (dict: query → answer)     │\n",
    "# └────────────────────────────────────────────┘\n",
    "#                   │\n",
    "#                   ▼\n",
    "# ┌────────────────────────────────────────────┐\n",
    "# │ ⚙️ Encode All field_variants Using Model    │\n",
    "# │ - SentenceTransformer(\"all-MiniLM-L6-v2\")  │\n",
    "# │ - Convert each query to semantic vector    │\n",
    "# │ - Save as: field_embeddings (tensor list)  │\n",
    "# └────────────────────────────────────────────┘\n",
    "#                   │\n",
    "#                   ▼\n",
    "# ┌────────────────────────────────────────────┐\n",
    "# │ 💾 Save Artifacts to Disk                   │\n",
    "# │ - df.pkl, field_map.pkl, field_variants.pkl│\n",
    "# │ - field_embeddings.pt                      │\n",
    "# └────────────────────────────────────────────┘\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "# ▶️ SYSTEM IS READY — USER ENTERS A QUERY BELOW:\n",
    "# ────────────────────────────────────────────────\n",
    "\n",
    "#                   │\n",
    "#                   ▼\n",
    "# ┌────────────────────────────────────────────┐\n",
    "# │ 🧍 USER INPUTS QUESTION (e.g., “Dad’s name?”) │\n",
    "# └────────────────────────────────────────────┘\n",
    "#                   │\n",
    "#                   ▼\n",
    "# ┌────────────────────────────────────────────┐\n",
    "# │ 🔧 Step 1: Preprocess Input                  │\n",
    "# │ - Lowercase                                 │\n",
    "# │ - Remove punctuation and extra whitespace   │\n",
    "# │ → \"Dad’s name?\" → \"dads name\"               │\n",
    "# └────────────────────────────────────────────┘\n",
    "#                   │\n",
    "#                   ▼\n",
    "# ┌────────────────────────────────────────────┐\n",
    "# │ 🔧 Step 2: Synonym Expansion (WordNet)      │\n",
    "# │ - For each word in input:                  │\n",
    "# │     • Add top 1–2 synonyms                 │\n",
    "# │ → \"dads name\" → \"dads name father dad\"     │\n",
    "# └────────────────────────────────────────────┘\n",
    "#                   │\n",
    "#                   ▼\n",
    "# ┌────────────────────────────────────────────┐\n",
    "# │ 🔧 Step 3: Encode Expanded Input            │\n",
    "# │ - Use same SentenceTransformer model       │\n",
    "# │ - Generate semantic embedding              │\n",
    "# └────────────────────────────────────────────┘\n",
    "#                   │\n",
    "#                   ▼\n",
    "# ┌────────────────────────────────────────────┐\n",
    "# │ 🔍 Step 4: Semantic Comparison              │\n",
    "# │ - Cosine similarity between user input &   │\n",
    "# │   each field_variant embedding             │\n",
    "# │ → Get best_match, best_score               │\n",
    "# └────────────────────────────────────────────┘\n",
    "#                   │\n",
    "#                   ▼\n",
    "# ┌────────────────────────────────────────────┐\n",
    "# │ 🔤 Step 5: Fuzzy String Matching            │\n",
    "# │ - Compare original input vs. best_match    │\n",
    "# │ - Use RapidFuzz `token_set_ratio()`        │\n",
    "# │ → Get fuzzy_score                          │\n",
    "# └────────────────────────────────────────────┘\n",
    "#                   │\n",
    "#                   ▼\n",
    "# ┌────────────────────────────────────────────┐\n",
    "# │ 🔎 Step 6: Check Match Thresholds           │\n",
    "# │ - If best_score ≥ 0.55  OR                 │\n",
    "# │   fuzzy_score ≥ 55                         │\n",
    "# │   → Proceed with best_match                │\n",
    "# │ - Else → Go to fallback response           │\n",
    "# └────────────────────────────────────────────┘\n",
    "#       │                            │\n",
    "#       ▼                            ▼\n",
    "# ┌────────────────────┐    ┌────────────────────────────┐\n",
    "# │ ✅ MATCH FOUND      │    │ ❌ NO CONFIDENT MATCH       │\n",
    "# └────────────────────┘    └────────────────────────────┘\n",
    "#       │                            │\n",
    "#       ▼                            ▼\n",
    "# ┌────────────────────────────────────────────┐\n",
    "# │ 🔁 Step 7: Retrieve Answer from field_map   │\n",
    "# │ - Lookup value using best_match            │\n",
    "# │ - Example: \"father name\" → \"Anshul Sharma\" │\n",
    "# └────────────────────────────────────────────┘\n",
    "#       │                            │\n",
    "#       ▼                            ▼\n",
    "# ┌────────────────────────────┐   ┌────────────────────────────────────────┐\n",
    "# │ 💬 Return Response:         │   │ 💬 Fallback:                           │\n",
    "# │   ✅ Matched: ‘father name’ │   │   🤖 Sorry, I’m not sure what you meant│\n",
    "# │   👉 Anshul Sharma          │   │   💡 Suggested closest: ‘father name’ │\n",
    "# └────────────────────────────┘   └────────────────────────────────────────┘\n",
    "#                   │\n",
    "#                   ▼\n",
    "# ┌────────────────────────────────────────────┐\n",
    "# │ 🔁 Loop: Wait for Next User Query or Exit   │\n",
    "# └────────────────────────────────────────────┘\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb062ab9-4159-422d-91cd-4ffbc4df5f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | 🔢 Index | 🧾 File               | 📦 Format         | 📌 Contents                                                                     | 🧠 Purpose                                                                                | 📂 Example                                                                                  |\n",
    "# | -------- | --------------------- | ----------------- | ------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------- |\n",
    "# | 1️⃣      | `df.pkl`              | Pickled DataFrame | A full table mapping each **field** (e.g., `Name`) to its **response**          | Used to show all available knowledge, help with UI listing, manual lookup, or export      | `{\"field\": \"Name\", \"value\": \"Amritanshu Mishra\"}`                                           |\n",
    "# | 2️⃣      | `field_map.pkl`       | Pickled dict      | Dictionary mapping every **user variant** to a **canonical field**              | Enables the bot to map fuzzy or alternative inputs to a consistent, known response source | `{\"your name\": \"Name\", \"who are you\": \"Name\"}` maps both to the `\"Name\"` field              |\n",
    "# | 3️⃣      | `field_variants.pkl`  | Pickled list      | A list of **all accepted phrases** or variants asked by users                   | Used as the raw text input to create sentence embeddings or apply fuzzy matching          | `[\"your name\", \"what's your full name\", \"who are you\", \"tell me your name\"]`                |\n",
    "# | 4️⃣      | `field_embeddings.pt` | PyTorch tensor    | A tensor with **vectorized embeddings** (e.g., SentenceTransformer) of variants | Allows fast **cosine similarity search** when user input doesn’t exactly match a variant  | Embedding for \"what's your full name\" stored as a 384-dim vector to match to `\"Name\"` field |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92ce2a7b-8cfe-45d3-aaa1-6f231177df0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\amrit\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "model.save('./local_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bf13cc9-ee24-47a5-8286-66bd2c98adbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a9d1149e7174682af1321461c1eb0ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amrit\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\amrit\\.cache\\huggingface\\hub\\models--sentence-transformers--paraphrase-MiniLM-L3-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58e5a5ba8bd74197b3028516a3772bb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e1554021d347b6a83ff26f755d390f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c49178407e5499cb00a3aedd884089a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9f489cf57934fb5a638dfc01d758d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "639ade0423ba45bea3a79a74dcb36a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/69.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "021abcd77ef347278adec0309f7b5e43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4d737e8d203476a8aa69eea875ca85a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59bc955238e64f0483f3dccafa8dd898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad4c4e44af9418b8e9b2cc780b44f3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f6758cb97344999843a2ac6499b406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L3-v2')\n",
    "model.save('./local_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4618844-f7a1-4935-9337-537be6665460",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
